{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "saItxTU2Vs9p"
   },
   "source": [
    "# Deep Learning 2019\n",
    "## Assignment 4 - Recurrent Neural Networks\n",
    "Please complete the questions below by modifying this notebook and send this file via e-mail to\n",
    "\n",
    "__[pir-assignments@l3s.de](mailto:pir-assignments@l3s.de?subject=[DL-2019]%20Assignment%20X%20[Name]%20[Mat.%20No.]&)__\n",
    "\n",
    "using the subject __[DL-2019] Assignment X [Name] [Mat. No.]__. The deadline for this assignment is __May 21st, 2019, 9AM__.\n",
    "\n",
    "Programming assignments have to be completed using Python 3. __Please do not use Python 2.__\n",
    "\n",
    "__Always explain your answers__ (do not just write 'yes' or 'no')."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B9Ar0ZUtVs9r"
   },
   "source": [
    "Please add your name and matriculation number below:\n",
    "\n",
    "__Name:__\n",
    "<br>\n",
    "__Mat. No.:__\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lj5q5JTHVs-b"
   },
   "source": [
    "### 1. Word Embeddings\n",
    "Consider a Word2Vec model with the vocabulary $\\{A, B, C, D, E\\}$ and the weight matrices\n",
    "\\begin{equation*}\n",
    "    W =\n",
    "        \\begin{pmatrix}\n",
    "            1 & -1  & 0\\\\\n",
    "            0 & 1   & 2\\\\\n",
    "            2 & 2   & 2\\\\\n",
    "            2 & 1   & 0\\\\\n",
    "            2 & -1  & 0\n",
    "        \\end{pmatrix}\n",
    "    \\quad \\text{and} \\quad\n",
    "    W' =\n",
    "        \\begin{pmatrix}\n",
    "            2 & 3   & 4   & 2 & 0\\\\\n",
    "            1 & 3   & -1  & 2 & 3\\\\\n",
    "            1 & -2  & 1   & 0 & 1\n",
    "        \\end{pmatrix}.\n",
    "\\end{equation*}\n",
    "Assume that the model uses the __CBOW__ architecture and that the one-hot indices correspond to the order of the words in the vocabulary above (the one-hot vector $(1, 0, 0, 0, 0)$ encodes the word $A$, the vector $(0, 1, 0, 0, 0)$ encodes $B$ and so on).\n",
    "1. What is one way the word $A$ can be embedded using these matrices?\n",
    "2. Suppose that at one point in time during training $W$ and $W'$ have the above values and the window is $B\\ C\\ D$. What would be the corresponding network input and output?\n",
    "3. What loss function is used in Word2Vec? What would be the inputs of the loss function in the given example?\n",
    "4. Compute the loss for the given example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AlsQ-gFiVs-d"
   },
   "source": [
    "### 2. Backpropagation through Time \n",
    "What happens to the gradient in vanilla RNNs if you backpropagate through a long sequence? What are some of the possible solutions proposed in literature to solve such problems?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Word2Vec\n",
    "The [Delicious Bookmarks](https://grouplens.org/datasets/hetrec-2011/) dataset contains URLs and corresponding tags from users.\n",
    "1. For every bookmark in the dataset, concatenate all tags and treat the resulting lists as sentences. Train a Word2Vec model with a window size of $5$ on these sentences to obtain $100$-dimensional word embeddings. Use the [gensim library](https://radimrehurek.com/gensim/models/word2vec.html) for this task.\n",
    "\n",
    "Now suppose we represent a bookmark $b$ that has a number of tags $T$ as the average of all word vectors, i.e.\n",
    "\\begin{equation}\n",
    "    v_b = \\frac{\\sum_{t \\in T} E(t)}{|T|}\n",
    "\\end{equation}\n",
    "where $E(t)$ is the embedding of $t$.\n",
    "\n",
    "2. Implement a simple search engine, where each bookmark is represented by a vector as described above. Queries are represented the same way. The relevance of a bookmark w.r.t. a query should be the cosine similarity between the two vectors. Print the top-$10$ results (the bookmark URLs) for the query `firefox addons extensions`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tl92kAaNVs9s"
   },
   "source": [
    "### 4. Sentiment Classification with LSTMs\n",
    "In this task we will implement a many-to-one LSTM to do sentiment classification. We will use the IMDB dataset, which is included in Keras. It contains movie reviews associated with sentiments (positive/negative). The task is to classify each review into one of these two classes.\n",
    "\n",
    "The training of this model requires a GPU. If you do not have one, we recommend using [Google CoLab](https://colab.research.google.com).\n",
    "\n",
    "If you get an error loading the dataset, try [downgrading to numpy 1.16.2](https://github.com/tensorflow/tensorflow/issues/28102):\n",
    "\n",
    "`$ pip uninstall numpy`\n",
    "\n",
    "`$ pip install numpy==1.16.2`\n",
    "\n",
    "We can use the `num_words` parameter to specify how many of the most frequent words should be included. The rest of the words will be replaced by a placeholder (`<UNK>`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "WWp1A9KYVs9u",
    "outputId": "29c24431-9cd9-4162-e3b2-d0de1ff3cf3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy==1.16.2 in /usr/local/lib/python3.6/dist-packages (1.16.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy==1.16.2\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "num_words = 1000\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.imdb.load_data(num_words=num_words, index_from=3)\n",
    "word_to_id = tf.keras.datasets.imdb.get_word_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dW17lGikVs92"
   },
   "source": [
    "The dataset has been preprocessed, i.e. stop words and punctuation marks were removed. Each word is assigned an index in the dictionary `word_to_id`. The sequences itself are made of indices instead of words. Since we'd like to translate the sequences back to words, we need to reverse the dictionary ([source](https://stackoverflow.com/questions/42821330/restore-original-text-from-keras-s-imdb-dataset))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "bpxRHzb0Vs93"
   },
   "outputs": [],
   "source": [
    "word_to_id = {k: v + 3 for k, v in word_to_id.items()}\n",
    "word_to_id['<PAD>'] = 0\n",
    "word_to_id['<START>'] = 1\n",
    "word_to_id['<UNK>'] = 2\n",
    "id_to_word = {value: key for key, value in word_to_id.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Afsc9sTXVs97"
   },
   "source": [
    "With this reverse index we can translate sequences of indices back to words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "colab_type": "code",
    "id": "Vu__yoyFVs98",
    "outputId": "acf288bb-c74d-49b2-a72c-64a196bee6a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 14, 22, 16, 43, 530, 973, 2, 2, 65, 458, 2, 66, 2, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 2, 2, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2, 19, 14, 22, 4, 2, 2, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 2, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2, 2, 16, 480, 66, 2, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 2, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 2, 15, 256, 4, 2, 7, 2, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 2, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 2, 88, 12, 16, 283, 5, 16, 2, 113, 103, 32, 15, 16, 2, 19, 178, 32]\n",
      "['<START>', 'this', 'film', 'was', 'just', 'brilliant', 'casting', '<UNK>', '<UNK>', 'story', 'direction', '<UNK>', 'really', '<UNK>', 'the', 'part', 'they', 'played', 'and', 'you', 'could', 'just', 'imagine', 'being', 'there', 'robert', '<UNK>', 'is', 'an', 'amazing', 'actor', 'and', 'now', 'the', 'same', 'being', 'director', '<UNK>', 'father', 'came', 'from', 'the', 'same', '<UNK>', '<UNK>', 'as', 'myself', 'so', 'i', 'loved', 'the', 'fact', 'there', 'was', 'a', 'real', '<UNK>', 'with', 'this', 'film', 'the', '<UNK>', '<UNK>', 'throughout', 'the', 'film', 'were', 'great', 'it', 'was', 'just', 'brilliant', 'so', 'much', 'that', 'i', '<UNK>', 'the', 'film', 'as', 'soon', 'as', 'it', 'was', 'released', 'for', '<UNK>', 'and', 'would', 'recommend', 'it', 'to', 'everyone', 'to', 'watch', 'and', 'the', '<UNK>', '<UNK>', 'was', 'amazing', 'really', '<UNK>', 'at', 'the', 'end', 'it', 'was', 'so', 'sad', 'and', 'you', 'know', 'what', 'they', 'say', 'if', 'you', '<UNK>', 'at', 'a', 'film', 'it', 'must', 'have', 'been', 'good', 'and', 'this', 'definitely', 'was', 'also', '<UNK>', 'to', 'the', 'two', 'little', '<UNK>', 'that', 'played', 'the', '<UNK>', 'of', '<UNK>', 'and', 'paul', 'they', 'were', 'just', 'brilliant', 'children', 'are', 'often', 'left', 'out', 'of', 'the', '<UNK>', '<UNK>', 'i', 'think', 'because', 'the', 'stars', 'that', 'play', 'them', 'all', '<UNK>', 'up', 'are', 'such', 'a', 'big', '<UNK>', 'for', 'the', 'whole', 'film', 'but', 'these', 'children', 'are', 'amazing', 'and', 'should', 'be', '<UNK>', 'for', 'what', 'they', 'have', 'done', \"don't\", 'you', 'think', 'the', 'whole', 'story', 'was', 'so', '<UNK>', 'because', 'it', 'was', 'true', 'and', 'was', '<UNK>', 'life', 'after', 'all', 'that', 'was', '<UNK>', 'with', 'us', 'all']\n"
     ]
    }
   ],
   "source": [
    "def get_text(seq):\n",
    "    return list(map(id_to_word.get, seq))\n",
    "\n",
    "print(x_train[0])\n",
    "print(get_text(x_train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lZBXUDDeVs-B"
   },
   "source": [
    "1. Define a many-to-one LSTM or GRU model that takes as input a sentence and classifies it as positive or negative. Experiment with different architectures (e.g. number of hidden units, dropout, `tf.keras.layers.Bidirectional` wrapper etc.). _Hint_: If you want to train on a GPU, use `tf.keras.layers.CuDNNLSTM` or `tf.keras.layers.CuDNNGRU`.\n",
    "2. Train your model. Use the best practices (validation, early stopping). Since the sequences are made out of numbers, you need to convert each number to a one-hot vector using `tf.keras.utils.to_categorical`. In order to use minibatching, all sequences in a batch must have the same length. You can use `tf.keras.preprocessing.sequence.pad_sequences` to pad the sequences with the `<PAD>` token (see above).\n",
    "3. Evaluate your trained model using `sklearn.metrics.classification_report` and `sklearn.metrics.confusion_matrix`. Compare two models, one with a vocabulary of $500$ words and one with $1000$ words."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "[DL-2019] Assignment 4 - Recurrent Neural Networks-solution.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
