{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to TensorFlow\n",
    "\n",
    "In this tutorial we will take a first look at the TensorFlow library as well as the built-in high-level API Keras. It is based on the [official TensorFlow guides and tutorials](https://www.tensorflow.org/overview/). The official documents are much more detailed, so it is always a good idea to go through them.\n",
    "\n",
    "Make sure you have a recent version of TensorFlow installed. Installation instructions can be found [here](https://www.tensorflow.org/install). If you have a beefy NVIDIA GPU, read [this](https://www.tensorflow.org/install/gpu). This tutorial is made with TF version 1.13.1. Note that TensorFlow 2.0 is on the horizon, which will introduce a bunch of cool [new features](https://medium.com/tensorflow/whats-coming-in-tensorflow-2-0-d3663832e9b8), however, as it is still in alpha at this point, we will stick to this version for now. Below you can check your installed version of TF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 1.13.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print('TensorFlow version: {}'.format(tf.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we have to enable eager execution, since it can only be done immediately after importing. This will be relevant later in the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential Models with Keras\n",
    "The easiest way of building deep learning models with TensorFlow is by using the high-level API Keras, which allows for easy definition of standard models in a sequential manner, i.e. by chaining together a number of layers. TensorFlow ships with its own implementation of Keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras version: 2.2.4-tf\n"
     ]
    }
   ],
   "source": [
    "print('Keras version: {}'.format(tf.keras.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Digit Recognition\n",
    "We will start by creating a feed-forward network for hand-written digit recognition using the [MNIST dataset](https://en.wikipedia.org/wiki/MNIST_database). It contains images, where each image shows one hand-written digit. The task is to classify each image into one of ten classes, one for each digit. This dataset is included in TensorFlow and can easily be loaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is already split in training and test sets. We can inspect the shape of the arrays to find out the number of examples and resolution of the images. In this case we have $60000$ training examples and $10000$ testing examples. The images are $28 \\times 28$ pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of x_train: (60000, 28, 28)\n",
      "shape of y_test: (10000,)\n"
     ]
    }
   ],
   "source": [
    "print('shape of x_train: {}'.format(x_train.shape))\n",
    "print('shape of y_test: {}'.format(y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In many cases it is useful to normalize the data. Our values are currently in $[0, 255]$. We normalize them to $[0, 1]$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can define our model. It is as simple as creating a `tf.keras.models.Sequential` object and adding a bunch of layers to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we need to to is flatten the input. Right now our images have a shape of $(28, 28)$. Since our model consists of regular fully-connected layers, we need to convert them to vectors. This can be done using a `tf.keras.layers.Flatten` layer, which is our model's input layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(tf.keras.layers.Flatten(input_shape=(28, 28)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can add some hidden layers. In our case we add a single dense (fully connected) hidden layer with $512$ neurons as well as some dropout regularization. In each layer we can specify the number of neurons and the activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jurek/.local/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:642: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/jurek/.local/lib/python3.6/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "model.add(tf.keras.layers.Dense(512, activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(rate=0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we add our output layer. Since we have $10$ classes, we need $10$ output neurons, where each neuron outputs the probability that the input image belongs to its class. Recall the definition of the _softmax_ function:\n",
    "\\begin{equation}\n",
    "\tsoftmax(y_i) = \\frac{e^{y_i}}{\\sum_{j=1}^{m} e^{y_j}}\n",
    "\\end{equation}\n",
    "Using the softmax as activation function produces a probability distribution in the output layer, where we can then select the class with the highest probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(tf.keras.layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our model is complete. We just need to choose a loss function and an optimizer. This can be done using the `compile` function. We choose the `adam` optimizer and cross entropy loss. We can also specify some metrics which will be reported during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After compiling the model, we can optionally print a summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 407,050\n",
      "Trainable params: 407,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we already have our data in the right format, training the model is as easy as calling the `fit` function. We can specify some parameters like the batch size or number of training epochs. After every epoch, it reports the loss and the metrics we specified earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 9s 156us/sample - loss: 0.2170 - acc: 0.9365\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 9s 144us/sample - loss: 0.0974 - acc: 0.9704\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 9s 145us/sample - loss: 0.0689 - acc: 0.9779\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 9s 142us/sample - loss: 0.0549 - acc: 0.9825\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 9s 143us/sample - loss: 0.0426 - acc: 0.9862\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7efe0c7d39e8>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, batch_size=32, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our model is trained, we can use it to make some predictions. Let's take the first item in the test set and predict its class. Note that our model expects batches as inputs, i.e. the shape should be $(b, 28, 28)$, where $b$ is the batch size (this can be variable). Even if we only want to input a single example, the shape must be $(1, 28, 28)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real label: 7\n",
      "Predicted probabilities: [1.5490368e-09 6.4304819e-09 7.0419937e-08 1.4518483e-06 6.6623326e-15\n",
      " 4.0967039e-09 4.0574503e-16 9.9999678e-01 9.4360297e-09 1.6224691e-06]\n",
      "Class with highest probability: 7 (0.9999967813491821)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print('Real label: {}'.format(y_test[0]))\n",
    "model_input = np.asarray([x_test[0]])\n",
    "prediction = model.predict(model_input)\n",
    "print('Predicted probabilities: {}'.format(prediction.flatten()))\n",
    "print('Class with highest probability: {} ({})'.format(np.argmax(prediction), np.max(prediction)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model assigned the image to the correct class with a probability of almost $100\\%$.\n",
    "\n",
    "We can evaluate the model on the test set using our metrics from earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0666 - acc: 0.9804\n",
      "Loss = 0.06656300846789381, accuracy = 0.980400025844574\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(x_test, y_test)\n",
    "print('Loss = {}, accuracy = {}'.format(loss, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations, you trained and evaluated your first Keras model! The trained weights can now be saved in a file with a simple command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('model_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading them back is just as easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0666 - acc: 0.9804\n",
      "Loss = 0.06656300846789381, accuracy = 0.980400025844574\n"
     ]
    }
   ],
   "source": [
    "model.load_weights('model_weights.h5')\n",
    "loss, acc = model.evaluate(x_test, y_test)\n",
    "print('Loss = {}, accuracy = {}'.format(loss, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the functions above only save the model's parameters, and not the model configuration itself. To save the complete model, use `save` and `tf.keras.models.load_model`. Note that this seems to be [currently bugged](https://github.com/tensorflow/tensorflow/issues/27688), and it throws a runtime error at the time of writing this tutorial, hence we don't show it here.\n",
    "\n",
    "### Validation\n",
    "So far we have only trained our model for a fixed number of epochs on our training data. However, this approach is very prone to overfitting. It is usually best to keep a smaller number of examples as a validation set (or dev set) and calculate the loss on this set after every epoch. As soon as we see the validation loss increase, we should stop the training to avoid overfitting.\n",
    "\n",
    "In order to tell Keras to stop after it has detected an increase in validation loss, we use a _callback_ function. These functions will be called after every training epoch. For early stopping, we can use the pre-made function `tf.keras.callbacks.EarlyStopping` to monitor the validation loss `val_loss`. We can also set the `patience` parameter, which describes the maximum number of epochs where an increase is tolerated before stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now start the training a new model, this time with validation data and early stopping. This time we set the number of epochs to a very high number to make sure we train until the validation loss stops decreasing. By setting `validation_split` to $0.1$ we automatically use $10\\%$ of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/100\n",
      "54000/54000 [==============================] - 8s 148us/sample - loss: 0.2318 - acc: 0.9302 - val_loss: 0.0985 - val_acc: 0.9705\n",
      "Epoch 2/100\n",
      "54000/54000 [==============================] - 8s 150us/sample - loss: 0.1008 - acc: 0.9685 - val_loss: 0.0845 - val_acc: 0.9762\n",
      "Epoch 3/100\n",
      "54000/54000 [==============================] - 8s 146us/sample - loss: 0.0717 - acc: 0.9775 - val_loss: 0.0656 - val_acc: 0.9795\n",
      "Epoch 4/100\n",
      "54000/54000 [==============================] - 8s 148us/sample - loss: 0.0542 - acc: 0.9830 - val_loss: 0.0665 - val_acc: 0.9798\n",
      "Epoch 5/100\n",
      "54000/54000 [==============================] - 9s 158us/sample - loss: 0.0457 - acc: 0.9854 - val_loss: 0.0647 - val_acc: 0.9810\n",
      "Epoch 6/100\n",
      "54000/54000 [==============================] - 8s 147us/sample - loss: 0.0374 - acc: 0.9879 - val_loss: 0.0640 - val_acc: 0.9820\n",
      "Epoch 7/100\n",
      "54000/54000 [==============================] - 8s 151us/sample - loss: 0.0328 - acc: 0.9889 - val_loss: 0.0607 - val_acc: 0.9833\n",
      "Epoch 8/100\n",
      "54000/54000 [==============================] - 8s 148us/sample - loss: 0.0265 - acc: 0.9911 - val_loss: 0.0798 - val_acc: 0.9797\n",
      "Epoch 9/100\n",
      "54000/54000 [==============================] - 8s 148us/sample - loss: 0.0245 - acc: 0.9917 - val_loss: 0.0615 - val_acc: 0.9840\n",
      "Epoch 10/100\n",
      "54000/54000 [==============================] - 8s 148us/sample - loss: 0.0235 - acc: 0.9921 - val_loss: 0.0765 - val_acc: 0.9795\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7efe0c7777f0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "                                    tf.keras.layers.Dense(512, activation='relu'),\n",
    "                                    tf.keras.layers.Dropout(rate=0.2),\n",
    "                                    tf.keras.layers.Dense(10, activation='softmax')])\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, batch_size=32, epochs=100, callbacks=[cb_early_stop], validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the validation loss starts to increase after just a couple of epochs and training stops accordingly.\n",
    "\n",
    "## Checkpoints\n",
    "Model checkpointing is an important technique when you train models over a longer period of time. You want to save your trained weights after every couple of epochs for two reasons:\n",
    "1. If the process is killed unexpectedly, you don't lose your progress.\n",
    "2. If your model starts to overfit, you can use the last 'good' checkpoint.\n",
    "\n",
    "In Keras, checkpoints are implemented through a callback function, `tf.keras.callbacks.ModelCheckpoint`. It will save checkpoints in a directory `model_ckpt` after every two epochs. The file name is important here, as it determines whether or not checkpoints will be overwritten. The file name in the example below will include the epoch and validation loss and make sure the checkpoints are not overwritten. We use `save_weights_only=True` here because of the aforementioned bug."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs('model_ckpt', exist_ok=True)\n",
    "filepath = os.path.join('model_ckpt', 'weights.{epoch:02d}-{val_loss:.2f}.hdf5')\n",
    "cb_ckpt = tf.keras.callbacks.ModelCheckpoint(filepath, period=2, save_weights_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, by setting the parameters `monitor='val_loss'` and `save_best_only=True`, we can keep just the checkpoint with the lowest validation loss.\n",
    "\n",
    "Let's train the same model yet again, this time with checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/100\n",
      "54000/54000 [==============================] - 8s 148us/sample - loss: 0.2331 - acc: 0.9304 - val_loss: 0.1064 - val_acc: 0.9675\n",
      "Epoch 2/100\n",
      "54000/54000 [==============================] - 8s 149us/sample - loss: 0.1012 - acc: 0.9699 - val_loss: 0.0813 - val_acc: 0.9762\n",
      "Epoch 3/100\n",
      "54000/54000 [==============================] - 8s 148us/sample - loss: 0.0729 - acc: 0.9774 - val_loss: 0.0695 - val_acc: 0.9792\n",
      "Epoch 4/100\n",
      "54000/54000 [==============================] - 9s 158us/sample - loss: 0.0550 - acc: 0.9826 - val_loss: 0.0643 - val_acc: 0.9812\n",
      "Epoch 5/100\n",
      "54000/54000 [==============================] - 9s 158us/sample - loss: 0.0450 - acc: 0.9854 - val_loss: 0.0754 - val_acc: 0.9783\n",
      "Epoch 6/100\n",
      "54000/54000 [==============================] - 8s 156us/sample - loss: 0.0372 - acc: 0.9873 - val_loss: 0.0641 - val_acc: 0.9825\n",
      "Epoch 7/100\n",
      "54000/54000 [==============================] - 8s 152us/sample - loss: 0.0327 - acc: 0.9891 - val_loss: 0.0729 - val_acc: 0.9797\n",
      "Epoch 8/100\n",
      "54000/54000 [==============================] - 8s 155us/sample - loss: 0.0265 - acc: 0.9911 - val_loss: 0.0786 - val_acc: 0.9803\n",
      "Epoch 9/100\n",
      "54000/54000 [==============================] - 8s 153us/sample - loss: 0.0242 - acc: 0.9922 - val_loss: 0.0825 - val_acc: 0.9795\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7efdec678f60>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "                                    tf.keras.layers.Dense(512, activation='relu'),\n",
    "                                    tf.keras.layers.Dropout(rate=0.2),\n",
    "                                    tf.keras.layers.Dense(10, activation='softmax')])\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, batch_size=32, epochs=100, callbacks=[cb_early_stop, cb_ckpt], validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorBoard\n",
    "TensorFlow ships with a great tool to monitor the training process and inspect the model. TensorBoard allows you to plot losses, metrics and other parameters like the learning rate in real time during the training and has many other cool features. In order to use it with our model, we have to export some data in log files while we train it. In Keras we can easily do this using, you guessed it, a callback, namely `tf.keras.callbacks.TensorBoard`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_tensorboard = tf.keras.callbacks.TensorBoard(log_dir='logs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the log files will be created in a directory called `logs`. After the training has started, you can launch TensorBoard by running the command\n",
    "\n",
    "`$ tensorboard --logdir=logs --host=localhost --port=1234`.\n",
    "\n",
    "Navigate your browser to http://localhost:1234 to access TensorBoard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/10\n",
      "54000/54000 [==============================] - 8s 156us/sample - loss: 0.2315 - acc: 0.9313 - val_loss: 0.0911 - val_acc: 0.9745\n",
      "Epoch 2/10\n",
      "54000/54000 [==============================] - 8s 151us/sample - loss: 0.1018 - acc: 0.9691 - val_loss: 0.0731 - val_acc: 0.9787\n",
      "Epoch 3/10\n",
      "54000/54000 [==============================] - 8s 155us/sample - loss: 0.0725 - acc: 0.9780 - val_loss: 0.0740 - val_acc: 0.9783\n",
      "Epoch 4/10\n",
      "54000/54000 [==============================] - 9s 163us/sample - loss: 0.0550 - acc: 0.9827 - val_loss: 0.0769 - val_acc: 0.9800\n",
      "Epoch 5/10\n",
      "54000/54000 [==============================] - 8s 154us/sample - loss: 0.0444 - acc: 0.9857 - val_loss: 0.0683 - val_acc: 0.9810\n",
      "Epoch 6/10\n",
      "54000/54000 [==============================] - 8s 152us/sample - loss: 0.0388 - acc: 0.9873 - val_loss: 0.0761 - val_acc: 0.9790\n",
      "Epoch 7/10\n",
      "54000/54000 [==============================] - 9s 168us/sample - loss: 0.0313 - acc: 0.9898 - val_loss: 0.0681 - val_acc: 0.9818\n",
      "Epoch 8/10\n",
      "54000/54000 [==============================] - 9s 161us/sample - loss: 0.0274 - acc: 0.9904 - val_loss: 0.0627 - val_acc: 0.9842\n",
      "Epoch 9/10\n",
      "54000/54000 [==============================] - 9s 160us/sample - loss: 0.0248 - acc: 0.9919 - val_loss: 0.0721 - val_acc: 0.9827\n",
      "Epoch 10/10\n",
      "54000/54000 [==============================] - 8s 153us/sample - loss: 0.0225 - acc: 0.9927 - val_loss: 0.0671 - val_acc: 0.9830\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7efdec1477b8>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "                                    tf.keras.layers.Dense(512, activation='relu'),\n",
    "                                    tf.keras.layers.Dropout(rate=0.2),\n",
    "                                    tf.keras.layers.Dense(10, activation='softmax')])\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, batch_size=32, epochs=10, callbacks=[cb_tensorboard], validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Models\n",
    "Should you ever need to build a more complex or non-standard model, the Keras API might not provide all the necessary layers for you. In this case it is necessary to get your hands dirty and create cutom models using a lower-level API. TensorFlow supports two evaluation strategies:\n",
    "\n",
    "1. ___Lazy evalution___ has been the default so far. Before doing any computations, all operations have to be defined in a _computation graph_. Afterwards the values of the graph's nodes can be calculated using some input data.\n",
    "\n",
    "2. ___Eager execution___ was introduced recently. It is very similar to regular imperative programming, i.e. computations are executed immediately instead of after finalizing the computation graph.\n",
    "\n",
    "Both strategies have their own advantages and disadvantages. In this tutorial we will only cover eager execution, as it is set to become the default in TensorFlow 2.\n",
    "\n",
    "### Basics\n",
    "The first thing we have to do is enable eager execution, which we did directly after importing TensorFlow. We can check whether eager execution is enabled using `tf.executing_eagerly()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(tf.executing_eagerly())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by trying out some basic math operations. For example, we can define a constant $2 \\times 2$ matrix using `tf.constant`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.constant([[1, 2],\n",
    "                 [3, 4]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is a `tf.Tensor` object.\n",
    "\n",
    "We can use regular Python operations on the matrices, for example `*` for element-wise multiplication or `@` for matrix multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a + a =\n",
      "[[2 4]\n",
      " [6 8]]\n",
      "\n",
      "a * a =\n",
      "[[ 1  4]\n",
      " [ 9 16]]\n",
      "\n",
      "a @ a =\n",
      "[[ 7 10]\n",
      " [15 22]]\n"
     ]
    }
   ],
   "source": [
    "print('a + a =\\n{}\\n'.format(a + a))\n",
    "print('a * a =\\n{}\\n'.format(a * a))\n",
    "print('a @ a =\\n{}'.format(a @ a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It also supports broadcasting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a + 1 =\n",
      "[[2 3]\n",
      " [4 5]]\n"
     ]
    }
   ],
   "source": [
    "print('a + 1 =\\n{}'.format(a + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the above operations do not mutate the Tensors, but create new ones each time.\n",
    "\n",
    "Trainable variables can be represented using `tf.Variable`. These variables can be assigned new values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(2, 2) dtype=int32, numpy=\n",
      "array([[1, 2],\n",
      "       [3, 4]], dtype=int32)>\n",
      "<tf.Variable 'Variable:0' shape=(2, 2) dtype=int32, numpy=\n",
      "array([[2, 4],\n",
      "       [6, 8]], dtype=int32)>\n"
     ]
    }
   ],
   "source": [
    "v = tf.Variable(a)\n",
    "print(v)\n",
    "\n",
    "v.assign(v + a)\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Linear Model\n",
    "We will now train a simple linear model using eager execution. Our model is a simple line with two parameters, the slope $m$ and the bias $b$. It produces an approximative function $f^*(x) = mx + b$.\n",
    "\n",
    "Our model class contains the two parameter attributes as variables, which are randomly initialized. If we call the model now we get random outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1.3388846], shape=(1,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "class Model(object):\n",
    "    def __init__(self):\n",
    "        self.m = tf.Variable(tf.random_uniform(shape=(1,)))\n",
    "        self.b = tf.Variable(tf.random_uniform(shape=(1,)))\n",
    "    def __call__(self, x):\n",
    "        return self.m * x + self.b\n",
    "\n",
    "model = Model()\n",
    "print(model(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function we want our model to approximate is $f(x) = 3x + 2$. We first generate some noisy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot\n",
    "\n",
    "def generate_data(f, spread, x_start, x_end, delta=1):\n",
    "    \"\"\"Generate a number of data points from f with some spread.\"\"\"\n",
    "    x = np.asarray(np.arange(x_start, x_end, delta))\n",
    "    y = f(x) + (np.random.rand(*x.shape) - 0.5) * spread\n",
    "    return x, y\n",
    "\n",
    "f = lambda x: 3 * x + 2\n",
    "x, y = generate_data(f, spread=5, x_start=0, x_end=10, delta=0.05)\n",
    "# plot the points\n",
    "pyplot.plot(x, y, marker='.', linewidth=0, label='data')\n",
    "# plot f\n",
    "pyplot.plot(x, f(x), linewidth=3, label='f(x)')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we have to define a loss function to minimize. In this case we choose the $L_2$ loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(predicted_y, desired_y):\n",
    "    return tf.reduce_mean(tf.square(predicted_y - desired_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the model, we implement the gradient descent algorithm. We use a `tf.GradientTape` object to record all operations that are applied to our variables. Afterwards we can use it to compute the gradients and update the variables. We train the model for $1000$ iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, inputs, outputs, learning_rate):\n",
    "    with tf.GradientTape() as t:\n",
    "        current_loss = loss(model(inputs), outputs)\n",
    "    dm, db = t.gradient(current_loss, [model.m, model.b])\n",
    "    model.m.assign_sub(learning_rate * dm)\n",
    "    model.b.assign_sub(learning_rate * db)\n",
    "\n",
    "for _ in range(1000):\n",
    "    train(model, x, y, 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the results, we plot our approximated function $f^*$ as well as the original function $f$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the points\n",
    "pyplot.plot(x, y, marker='.', linewidth=0, label='data')\n",
    "# plot f\n",
    "pyplot.plot(x, f(x), linewidth=3, label='f(x)')\n",
    "# plot f*\n",
    "pyplot.plot(x, model(x), linewidth=3, label='f*(x)')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Keras Layers\n",
    "It is also possible to subclass `tf.keras.layers.Layer` in order to define your own Keras layers. These can then be used in sequential models. Simply override the methods `__init__`, `build` and `call`. The `build` function is called once when the layer is used for the first time, which provides us with the input shape (depending on the previous layer). In the example below we implement a simpler version of `tf.keras.layers.Dense`, i.e. a dense layer with just a weight matrix but no bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySimpleLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, output_units):\n",
    "        super(MySimpleLayer, self).__init__()\n",
    "        self.output_units = output_units\n",
    "    def build(self, input_shape):\n",
    "        self.kernel = self.add_variable('kernel', [input_shape[-1], self.output_units])\n",
    "    def call(self, input):\n",
    "        return tf.matmul(input, self.kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Keras Models\n",
    "Additionally it is possible to subclass `tf.keras.Model` to build a custom model. This is useful if you need a model which is not strictly sequential. We again override `__init__` and `call`. Here we simply call the individual layers to apply them to some inputs. This allows us for example to apply the same layer twice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.dense1 = tf.keras.layers.Dense(units=10)\n",
    "        self.dense2 = tf.keras.layers.Dense(units=10)\n",
    "    def call(self, input):\n",
    "        result = self.dense1(input)\n",
    "        result = self.dense2(result)\n",
    "        result = self.dense2(result)  # reuse variables from dense2 layer\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Training\n",
    "Finally, we will use eager execution mode to train our MNIST model from before. Note that we removed the softmax activation from the last layer as there is a loss function which takes care of this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "                                    tf.keras.layers.Dense(512, activation='relu'),\n",
    "                                    tf.keras.layers.Dropout(rate=0.2),\n",
    "                                    tf.keras.layers.Dense(10)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first create a function that yields batches of our training data. We cast the images to float and the labels to int, as this is expected by the loss function we will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(x, y, batch_size):\n",
    "    assert len(x) == len(y)\n",
    "    for i in range(0, len(x), batch_size):\n",
    "        yield tf.cast(x[i:i + batch_size], tf.float32), tf.cast(y[i:i + batch_size], tf.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we will use a pre-implemented optimizer (`tf.train.AdamOptimizer`). For every batch we run the forward pass to obtain the logits. We then compute the cross entropy loss. Finally we calculate and apply the gradients using `tf.GradientTape` and the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jurek/.local/lib/python3.6/site-packages/tensorflow/python/ops/losses/losses_impl.py:209: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.train.AdamOptimizer()\n",
    "for images, labels in get_batches(x_train, y_train, 32):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(images, training=True)\n",
    "        loss_value = tf.losses.sparse_softmax_cross_entropy(labels, logits)\n",
    "    grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables),\n",
    "                              global_step=tf.train.get_or_create_global_step())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Reading\n",
    "In this tutorial we only scratched the surface on what's possible with TensorFlow. Below are some links about advanced topics.\n",
    "* [Estimators](https://www.tensorflow.org/guide/estimators) - another high-level API\n",
    "* [`tf.data`](https://www.tensorflow.org/guide/datasets) - import data\n",
    "* [TensorFlow Serving](https://github.com/tensorflow/serving) - serve trained models\n",
    "* [TensorFlow Hub](https://www.tensorflow.org/hub) - share TensorFlow models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
