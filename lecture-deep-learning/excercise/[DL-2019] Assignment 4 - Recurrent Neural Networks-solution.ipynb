{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "saItxTU2Vs9p"
   },
   "source": [
    "# Deep Learning 2019\n",
    "## Assignment 4 - Recurrent Neural Networks\n",
    "Please complete the questions below by modifying this notebook and send this file via e-mail to\n",
    "\n",
    "__[pir-assignments@l3s.de](mailto:pir-assignments@l3s.de?subject=[DL-2019]%20Assignment%20X%20[Name]%20[Mat.%20No.]&)__\n",
    "\n",
    "using the subject __[DL-2019] Assignment X [Name] [Mat. No.]__. The deadline for this assignment is __May 21st, 2019, 9AM__.\n",
    "\n",
    "Programming assignments have to be completed using Python 3. __Please do not use Python 2.__\n",
    "\n",
    "__Always explain your answers__ (do not just write 'yes' or 'no')."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B9Ar0ZUtVs9r"
   },
   "source": [
    "Please add your name and matriculation number below:\n",
    "\n",
    "__Name:__\n",
    "<br>\n",
    "__Mat. No.:__\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lj5q5JTHVs-b"
   },
   "source": [
    "### 1. Word Embeddings\n",
    "Consider a Word2Vec model with the vocabulary $\\{A, B, C, D, E\\}$ and the weight matrices\n",
    "\\begin{equation*}\n",
    "    W =\n",
    "        \\begin{pmatrix}\n",
    "            1 & -1  & 0\\\\\n",
    "            0 & 1   & 2\\\\\n",
    "            2 & 2   & 2\\\\\n",
    "            2 & 1   & 0\\\\\n",
    "            2 & -1  & 0\n",
    "        \\end{pmatrix}\n",
    "    \\quad \\text{and} \\quad\n",
    "    W' =\n",
    "        \\begin{pmatrix}\n",
    "            2 & 3   & 4   & 2 & 0\\\\\n",
    "            1 & 3   & -1  & 2 & 3\\\\\n",
    "            1 & -2  & 1   & 0 & 1\n",
    "        \\end{pmatrix}.\n",
    "\\end{equation*}\n",
    "Assume that the model uses the __CBOW__ architecture and that the one-hot indices correspond to the order of the words in the vocabulary above (the one-hot vector $(1, 0, 0, 0, 0)$ encodes the word $A$, the vector $(0, 1, 0, 0, 0)$ encodes $B$ and so on).\n",
    "1. What is one way the word $A$ can be embedded using these matrices?\n",
    "2. Suppose that at one point in time during training $W$ and $W'$ have the above values and the window is $B\\ C\\ D$. What would be the corresponding network input and output?\n",
    "3. What loss function is used in Word2Vec? What would be the inputs of the loss function in the given example?\n",
    "4. Compute the loss for the given example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Tt0b8oP5Vs-c"
   },
   "source": [
    "#### Solution\n",
    "1. The simplest way of embedding a single word is by using its hidden representation in the network. Because we are using one-hot representations, this corresponds to a single row in $W$. In the case of $A$ we get $(1, -1, 0)$.\n",
    "2. In the CBOW architecture the network predicts the word in the center (target) given all other words in the window (context). In our case the target word is $C$, and thus the expected output is $(0, 0, 1, 0, 0)$. The input is the average of the embeddings of the context words $B$ and $D$, i.e. $(0, 0.5, 0, 0.5, 0)$. We can now calculate the actual network output using the input and the weights as $ŷ = x^T \\cdot W \\cdot W' = (4, 4, 4, 4, 4)$.\n",
    "3. Since this problem can be seen as a prediction task (with many classes), we use cross-entropy loss. The inputs of the loss function are the model output after applying the softmax function and the expected output, i.e. $softmax(ŷ) = (0.2, 0.2, 0.2, 0.2, 0.2)$ and $(0, 0, 1, 0, 0)$.\n",
    "4. We can use the function integrated in Keras to calculate the cross-entropy. Note that the parameter `from_logits=True` means that the softmax will be applied before the loss is computed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.60943791]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.InteractiveSession()\n",
    "\n",
    "y = np.asarray([[0, 0, 1, 0, 0]])\n",
    "y_pred = np.asarray([[4., 4., 4., 4., 4.]])\n",
    "\n",
    "print(tf.keras.backend.categorical_crossentropy(y, y_pred, from_logits=True).eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AlsQ-gFiVs-d"
   },
   "source": [
    "### 2. Backpropagation through Time \n",
    "What happens to the gradient in vanilla RNNs if you backpropagate through a long sequence? What are some of the possible solutions proposed in literature to solve such problems?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution\n",
    "\n",
    "The gradients coming from the deeper layers in RNNs have to go through continuous matrix multiplications because of the the chain rule, and as they approach the earlier layers, if they have small values $(<1)$, they shrink exponentially until they vanish and make it impossible for the model to learn. This is the vanishing gradient problem. On the other hand, if they have large values $(>1)$ they get larger and eventually blow up. This is the exploding gradient problem.\n",
    "\n",
    "The following are some of the possible solutions to mitigate such problems:\n",
    "1. Gradient Clipping: When gradients explode, they can become NaNs because of the numerical overflow or we might see irregular oscillations in training cost when we plot the learning curve. A solution to fix this is to apply gradient clipping, which places a predefined threshold on the gradients to prevent it from getting too large. This doesn’t change the direction of the gradients, only their lengths.\n",
    "2. In LSTM and GRU, each unit consists of \"gates\" which are applied to the previous state, input vector, output state and/or candidate output vectors. This allows the unit to control the influence that previous states have on the current state, thereby allowing it to control the magnitude of gradient propagation.\n",
    "3. Regularize the parameters to avoid vanishing gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Word2Vec\n",
    "The [Delicious Bookmarks](https://grouplens.org/datasets/hetrec-2011/) dataset contains URLs and corresponding tags from users.\n",
    "1. For every bookmark in the dataset, concatenate all tags and treat the resulting lists as sentences. Train a Word2Vec model with a window size of $5$ on these sentences to obtain $100$-dimensional word embeddings. Use the [gensim library](https://radimrehurek.com/gensim/models/word2vec.html) for this task.\n",
    "\n",
    "Now suppose we represent a bookmark $b$ that has a number of tags $T$ as the average of all word vectors, i.e.\n",
    "\\begin{equation}\n",
    "    v_b = \\frac{\\sum_{t \\in T} E(t)}{|T|}\n",
    "\\end{equation}\n",
    "where $E(t)$ is the embedding of $t$.\n",
    "\n",
    "2. Implement a simple search engine, where each bookmark is represented by a vector as described above. Queries are represented the same way. The relevance of a bookmark w.r.t. a query should be the cosine similarity between the two vectors. Print the top-$10$ results (the bookmark URLs) for the query `firefox addons extensions`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from gensim.models import Word2Vec\n",
    "import pandas\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use pandas to read the files. We end up with two dictionaries `tags` and `urls` that map IDs to tags and URLs. `bookmarks_tags` contains tuples of bookmark IDs, tag IDs and tag weights. We ignore the weights and concatenate the tags for each bookmark ID to obtain the sentences for Word2Vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tags = pandas.read_csv('tags.dat', encoding='latin-1', delimiter='\\t').set_index('id').to_dict()['value']\n",
    "urls = pandas.read_csv('bookmarks.dat', encoding='latin-1', delimiter='\\t').set_index('id').to_dict()['url']\n",
    "bookmarks_tags = pandas.read_csv('bookmark_tags.dat', encoding='latin-1', delimiter='\\t').values\n",
    "\n",
    "sentences = defaultdict(list)\n",
    "for b_id, t_id, _ in bookmarks_tags:\n",
    "    if t_id < len(tags):\n",
    "        sentences[b_id].append(tags[t_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train a Word2Vec model on these sentences and save the sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 100\n",
    "model = Word2Vec(sentences.values(), size=EMBEDDING_SIZE, window=5)\n",
    "word_vectors = model.wv\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We represent each bookmark as the average of the embedding vectors of its tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docs = defaultdict(lambda: numpy.zeros([EMBEDDING_SIZE]))\n",
    "for b_id, tags in sentences.items():\n",
    "    num_tags = 0\n",
    "    for tag in tags:\n",
    "        if tag in word_vectors:\n",
    "            num_tags += 1\n",
    "            docs[b_id] += word_vectors[tag]\n",
    "    if num_tags > 0:\n",
    "        docs[b_id] /= num_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We represent the query the same way as the bookmarks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query = 'firefox addons extensions'\n",
    "query_vec = numpy.zeros([EMBEDDING_SIZE])\n",
    "num = 0\n",
    "for word in query.split():\n",
    "    if word in word_vectors:\n",
    "        num += 1\n",
    "        query_vec += word_vectors[word]\n",
    "query_vec /= num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we compute the similarities for all bookmarks and print the top-$10$ results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top-10 results:\n",
      "1\tscore: 0.998458575326554\t55827\thttps://addons.mozilla.org/en-US/firefox/addon/11233\n",
      "2\tscore: 0.9910288703149589\t45170\thttps://chrome.google.com/extensions/detail/djnnmjiciadfjbpoclahceniaoeiabbb?hl=es\n",
      "3\tscore: 0.9907319104315977\t53013\thttps://addons.mozilla.org/en-US/firefox/addon/98440\n",
      "4\tscore: 0.9888803425230343\t105817\thttps://addons.mozilla.org/en-US/firefox/addon/3179\n",
      "5\tscore: 0.988613161444\t19834\thttps://addons.mozilla.org/es-ES/firefox/addon/14971/\n",
      "6\tscore: 0.9878646184918128\t61522\thttp://xsticky.com/\n",
      "7\tscore: 0.9869709338469045\t59013\thttp://www.seoquake.com/\n",
      "8\tscore: 0.9867048189249178\t19616\thttp://orera.g.hatena.ne.jp/edvakf/20101021/1287659747\n",
      "9\tscore: 0.9833431323905616\t38240\thttp://www.clubic.com/navigateur-internet/mozilla-firefox/article-374036-1-extension-naviguer-internet-firefox-chrome-safari.html\n",
      "10\tscore: 0.9807386104866544\t19480\thttp://vimperator.g.hatena.ne.jp/teramako/20100921/1285079275\n"
     ]
    }
   ],
   "source": [
    "def cosine_sim(u, v):\n",
    "    return numpy.dot(u, v) / numpy.linalg.norm(u) / numpy.linalg.norm(v)\n",
    "\n",
    "scores = []\n",
    "for b_id, doc in docs.items():\n",
    "    scores.append((b_id, cosine_sim(doc, query_vec)))\n",
    "scores.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print('top-10 results:')\n",
    "for i in range(10):\n",
    "    b_id, score = scores[i]\n",
    "    print('{}\\tscore: {}\\t{}\\t{}'.format(i + 1, score, b_id, urls.get(b_id)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tl92kAaNVs9s"
   },
   "source": [
    "### 4. Sentiment Classification with LSTMs\n",
    "In this task we will implement a many-to-one LSTM to do sentiment classification. We will use the IMDB dataset, which is included in Keras. It contains movie reviews associated with sentiments (positive/negative). The task is to classify each review into one of these two classes.\n",
    "\n",
    "The training of this model requires a GPU. If you do not have one, we recommend using [Google CoLab](https://colab.research.google.com).\n",
    "\n",
    "If you get an error loading the dataset, try [downgrading to numpy 1.16.2](https://github.com/tensorflow/tensorflow/issues/28102):\n",
    "\n",
    "`$ pip uninstall numpy`\n",
    "\n",
    "`$ pip install numpy==1.16.2`\n",
    "\n",
    "We can use the `num_words` parameter to specify how many of the most frequent words should be included. The rest of the words will be replaced by a placeholder (`<UNK>`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "WWp1A9KYVs9u",
    "outputId": "29c24431-9cd9-4162-e3b2-d0de1ff3cf3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy==1.16.2 in /usr/local/lib/python3.6/dist-packages (1.16.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy==1.16.2\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "num_words = 1000\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.imdb.load_data(num_words=num_words, index_from=3)\n",
    "word_to_id = tf.keras.datasets.imdb.get_word_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dW17lGikVs92"
   },
   "source": [
    "The dataset has been preprocessed, i.e. stop words and punctuation marks were removed. Each word is assigned an index in the dictionary `word_to_id`. The sequences itself are made of indices instead of words. Since we'd like to translate the sequences back to words, we need to reverse the dictionary ([source](https://stackoverflow.com/questions/42821330/restore-original-text-from-keras-s-imdb-dataset))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "bpxRHzb0Vs93"
   },
   "outputs": [],
   "source": [
    "word_to_id = {k: v + 3 for k, v in word_to_id.items()}\n",
    "word_to_id['<PAD>'] = 0\n",
    "word_to_id['<START>'] = 1\n",
    "word_to_id['<UNK>'] = 2\n",
    "id_to_word = {value: key for key, value in word_to_id.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Afsc9sTXVs97"
   },
   "source": [
    "With this reverse index we can translate sequences of indices back to words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "colab_type": "code",
    "id": "Vu__yoyFVs98",
    "outputId": "acf288bb-c74d-49b2-a72c-64a196bee6a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 14, 22, 16, 43, 530, 973, 2, 2, 65, 458, 2, 66, 2, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 2, 2, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2, 19, 14, 22, 4, 2, 2, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 2, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2, 2, 16, 480, 66, 2, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 2, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 2, 15, 256, 4, 2, 7, 2, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 2, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 2, 88, 12, 16, 283, 5, 16, 2, 113, 103, 32, 15, 16, 2, 19, 178, 32]\n",
      "['<START>', 'this', 'film', 'was', 'just', 'brilliant', 'casting', '<UNK>', '<UNK>', 'story', 'direction', '<UNK>', 'really', '<UNK>', 'the', 'part', 'they', 'played', 'and', 'you', 'could', 'just', 'imagine', 'being', 'there', 'robert', '<UNK>', 'is', 'an', 'amazing', 'actor', 'and', 'now', 'the', 'same', 'being', 'director', '<UNK>', 'father', 'came', 'from', 'the', 'same', '<UNK>', '<UNK>', 'as', 'myself', 'so', 'i', 'loved', 'the', 'fact', 'there', 'was', 'a', 'real', '<UNK>', 'with', 'this', 'film', 'the', '<UNK>', '<UNK>', 'throughout', 'the', 'film', 'were', 'great', 'it', 'was', 'just', 'brilliant', 'so', 'much', 'that', 'i', '<UNK>', 'the', 'film', 'as', 'soon', 'as', 'it', 'was', 'released', 'for', '<UNK>', 'and', 'would', 'recommend', 'it', 'to', 'everyone', 'to', 'watch', 'and', 'the', '<UNK>', '<UNK>', 'was', 'amazing', 'really', '<UNK>', 'at', 'the', 'end', 'it', 'was', 'so', 'sad', 'and', 'you', 'know', 'what', 'they', 'say', 'if', 'you', '<UNK>', 'at', 'a', 'film', 'it', 'must', 'have', 'been', 'good', 'and', 'this', 'definitely', 'was', 'also', '<UNK>', 'to', 'the', 'two', 'little', '<UNK>', 'that', 'played', 'the', '<UNK>', 'of', '<UNK>', 'and', 'paul', 'they', 'were', 'just', 'brilliant', 'children', 'are', 'often', 'left', 'out', 'of', 'the', '<UNK>', '<UNK>', 'i', 'think', 'because', 'the', 'stars', 'that', 'play', 'them', 'all', '<UNK>', 'up', 'are', 'such', 'a', 'big', '<UNK>', 'for', 'the', 'whole', 'film', 'but', 'these', 'children', 'are', 'amazing', 'and', 'should', 'be', '<UNK>', 'for', 'what', 'they', 'have', 'done', \"don't\", 'you', 'think', 'the', 'whole', 'story', 'was', 'so', '<UNK>', 'because', 'it', 'was', 'true', 'and', 'was', '<UNK>', 'life', 'after', 'all', 'that', 'was', '<UNK>', 'with', 'us', 'all']\n"
     ]
    }
   ],
   "source": [
    "def get_text(seq):\n",
    "    return list(map(id_to_word.get, seq))\n",
    "\n",
    "print(x_train[0])\n",
    "print(get_text(x_train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lZBXUDDeVs-B"
   },
   "source": [
    "1. Define a many-to-one LSTM or GRU model that takes as input a sentence and classifies it as positive or negative. Experiment with different architectures (e.g. number of hidden units, dropout, `tf.keras.layers.Bidirectional` wrapper etc.). _Hint_: If you want to train on a GPU, use `tf.keras.layers.CuDNNLSTM` or `tf.keras.layers.CuDNNGRU`.\n",
    "2. Train your model. Use the best practices (validation, early stopping). Since the sequences are made out of numbers, you need to convert each number to a one-hot vector using `tf.keras.utils.to_categorical`. In order to use minibatching, all sequences in a batch must have the same length. You can use `tf.keras.preprocessing.sequence.pad_sequences` to pad the sequences with the `<PAD>` token (see above).\n",
    "3. Evaluate your trained model using `sklearn.metrics.classification_report` and `sklearn.metrics.confusion_matrix`. Compare two models, one with a vocabulary of $500$ words and one with $1000$ words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lnU7_mWcVs-C"
   },
   "source": [
    "#### Solution\n",
    "We use a simple model with an LSTM and dropout. Our output is just a single number indicating the probability that the sentence was positive (class $1$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "-WA_SRrBVs-E"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.InputLayer(input_shape=(None, num_words + 3)),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.CuDNNLSTM(128, return_sequences=False)),\n",
    "    tf.keras.layers.Dropout(rate=0.5),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qThkoRoLVs-I"
   },
   "source": [
    "We create a batch generator that yields batches from the data, converted to one-hot vectors and padded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "YP688NFFVs-J"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def convert_to_onehot(x, num_classes):\n",
    "    result = []\n",
    "    for x_ in x:\n",
    "        result.append(tf.keras.utils.to_categorical(x_, num_classes=num_classes))\n",
    "    return result\n",
    "\n",
    "def batch_generator(x_train, y_train, batch_size, num_classes):\n",
    "    assert len(x_train) == len(y_train)\n",
    "    while True:\n",
    "        i = 0\n",
    "        while i < len(x_train) - batch_size:\n",
    "            b_x = x_train[i:i + batch_size]\n",
    "            b_y = y_train[i:i + batch_size]\n",
    "            b_x_oh = convert_to_onehot(b_x, num_classes)\n",
    "            b_x_pad = tf.keras.preprocessing.sequence.pad_sequences(b_x_oh, value=word_to_id['<PAD>'])\n",
    "            yield b_x_pad, b_y\n",
    "            i += batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zZS5KX86Vs-N"
   },
   "source": [
    "In order to use validation, we need to split the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "p_OYkwnWVs-O"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2O3tHy-LVs-R"
   },
   "source": [
    "Now we can start training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 251
    },
    "colab_type": "code",
    "id": "RScWN6ueVs-S",
    "outputId": "53df2ac8-3743-4486-d898-527965fcf214",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "78/78 [==============================] - 10s 127ms/step - loss: 0.4523\n",
      "703/703 [==============================] - 131s 187ms/step - loss: 0.6007 - val_loss: 0.4523\n",
      "Epoch 2/100\n",
      "78/78 [==============================] - 10s 130ms/step - loss: 0.3967\n",
      "703/703 [==============================] - 130s 185ms/step - loss: 0.4250 - val_loss: 0.3967\n",
      "Epoch 3/100\n",
      "78/78 [==============================] - 10s 125ms/step - loss: 0.4797\n",
      "703/703 [==============================] - 129s 184ms/step - loss: 0.6684 - val_loss: 0.4797\n",
      "Epoch 4/100\n",
      "78/78 [==============================] - 9s 121ms/step - loss: 0.5034\n",
      "703/703 [==============================] - 130s 185ms/step - loss: 0.5368 - val_loss: 0.5034\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9d483313c8>"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cb_early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2)\n",
    "\n",
    "batch_size = 32\n",
    "train_gen = batch_generator(x_train, y_train, batch_size, num_words + 3)\n",
    "validation_gen = batch_generator(x_val, y_val, batch_size, num_words + 3)\n",
    "steps = int(len(x_train) / batch_size)\n",
    "val_steps = int(len(x_val) / batch_size)\n",
    "model.fit_generator(train_gen, epochs=100, steps_per_epoch=steps, callbacks=[cb_early_stop],\n",
    "                    validation_data=validation_gen, validation_steps=val_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-hZzS30xVs-W"
   },
   "source": [
    "Finally we predict the labels of the test set and evaluate the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 215
    },
    "colab_type": "code",
    "id": "ArbnUk1RVs-X",
    "outputId": "41ff2505-3d25-4d09-9865-d5b2d9517778"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.64      0.72     12500\n",
      "           1       0.71      0.86      0.77     12500\n",
      "\n",
      "   micro avg       0.75      0.75      0.75     25000\n",
      "   macro avg       0.76      0.75      0.75     25000\n",
      "weighted avg       0.76      0.75      0.75     25000\n",
      "\n",
      "[[ 8052  4448]\n",
      " [ 1793 10707]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "y_pred = []\n",
    "for x in x_test:\n",
    "    y, = model.predict([convert_to_onehot([x], num_words + 3)])\n",
    "    y_pred.append(y)\n",
    "\n",
    "y_pred = np.around(y_pred)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We skip the comparison with the other model here since it can be easily trained by changing the `num_words` variable."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "[DL-2019] Assignment 4 - Recurrent Neural Networks-solution.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
